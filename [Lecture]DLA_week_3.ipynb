{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9FQOnx1cmiwiqFhPaUvO+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["실습 유의점\n","1. 데이터 불러올 때 index_col=0 추가함으로써 undefined 값 제거 <br>\n","2. 데이터 불러온 후 제일 먼저 하는 일은 label 확인하기 (모든 라벨이 순차적인지 확인)<br>\n","3. 모든 라벨이 균등하지 않은 경우 숫자를 숫자로 다시 인코딩 <br>\n","4. 넣어도 넣지 말아도 좋은 데이터는 넣지 말 것 <- 차원의 저주 때문(적은 데이터롤 효율적으로 학습이 목표)"],"metadata":{"id":"cNrqPG0UkM0r"}},{"cell_type":"markdown","source":["영향을 미치지 않는 label은 오히려 noise처럼 작동한다 <br>\n","그러니 그런 label을 잘 판별해서 제외하는 것이 중요하다\n","<br> feature-selection을 위해 모든 경우의 수를 따지는 것은 비효율적 -> repo 대신 알고리즘 등을 사용하여 거른다 <br>\n","매니폴드 <- 종이를 구겼을 때의 최단거리를 구하는 것보다 펴서 최단거리를 구하는 것이 쉽고 정확도 높다 = 저차원화 시켜서(차원 축소) 데이터 간의 거리를 구한다\n","<br>"],"metadata":{"id":"nRIm3I3Ip_PH"}},{"cell_type":"markdown","source":["주성분 분석 -> 차원 축소를 위한 방법 중 하나 <br>\n","사이킷런의 PCA 모델이다 <br>\n","비지도학습은 어렵고 오래 걸리지만 레이블을 알아서 생성하기에 자주 한다 <br>"],"metadata":{"id":"dOmISKpQtFTL"}},{"cell_type":"markdown","source":["딥러닝에서 필요한 것\n","1. one-hot encoding\n","2. optimizer\n","3. loss function\n","4. forward / backward propagation\n","5. activation fuction"],"metadata":{"id":"ITkvrAhFtyjH"}},{"cell_type":"markdown","source":["skip-connecion -> 은닉층 등 중간 층을 건너뛰고 특정 층을 입력값과 다시 연결해서 출력에 더해준다 <br>알기로는 gradient vanishing을 막기 위한 것 아니었나 싶은데 정확하지 않음 <= 이거 맞음 <br>\n","backpropagation 등에서 미분하다가 값이 소실됨 -> gradient vanishing(깊은 네트워크를 사용하다가 미분값이 소실됨)\n","\n","<br>\n","gradient vanishing problem solving\n","<- f(x)를 구하는 방법은 어려우니 간접적으로 이용한다\n","\n","skip-connection을 하는 이유\n","1. 입력과 특정 층의 차이를 구해서 출력 층에 더해준다 <- gradient vanishing 보정\n","2. 잔차로 학습을 쉽게 한다\n","\n","* input이 여러 개인 경우 : fit 할 때 각자 입력, 출력을 여러 개 넣으면 한 모델로 여러 출력 확인 가능(나중에 해볼 것)\n","* model save를 하면 컴퓨터 켤 때마다 학습할 필요 없이 저장 가능 but 그게 항상 좋은 것은 아님 <- 학습 할 때마다 결과가 달라지기 때문 가끔은 그냥 성능이 많이 변하기도 함(이유 모름...)"],"metadata":{"id":"KzKJ57-0vgDS"}},{"cell_type":"markdown","source":["시험문제 키워드\n","1장 : 머신러닝 딥러닝 차이점\n","차원 축소 feature-selection 자가지능학습, 마스킹,오버피팅, 언더피팅, cross-validation, mse 수식(p75), crossentropy, F&Q에 있는 건 그냥 다 나옴\n","K-FOLD running, manyfold running ,정밀도,재현도, accuracy, ROC(그냥 보기만 할 것), 경사하강법 전반(전체 경사하강법, 확률적 경사하강법,미니 배치 경사하강법)(192p), l1,l2, 슈퍼백터머신(슈퍼백터), 지니계수와 지니불순도, bagging과 testing 차이(237p) 앙상블 차원축소 manybold running <br> 다음 시간에는 그냥 2권만 가지고 올 것\n"],"metadata":{"id":"PVIvZY4o1JZQ"}},{"cell_type":"markdown","source":["activation function도 직접 만들 수 있다... <br>\n","optimizer도 가능 <br>\n","* 13장 거의 스킵(잔차 블럭만 조금 보자)\n","* CNN과 Full-connected-layer의 차이점\n","* NN : picture를 쪼개서 pixel로 인식 -> 그 과정에서 loacal의 관계들이 소실됨(그저 데이터로 인식, 특정 패턴 등을 파악x)\n","* 그래서 이를 해결하기 위해 local feature 이용 <- CNN(Convolusion neural network)\n","* 뒤의 학습이 훨씬 feature를 잘 거르므로 앞의 layer들 말고 뒤의 신경망만 재사용하자! -> 전이학습(transfer-learning),(pre-trained) 등\n","* pooling을 하는 이유 :\n","  * 정보를 압축(요약)\n","  * 평행 이동에 대한 방지(불변하는 인식? 인식의 불변성 제공)\n","\n"],"metadata":{"id":"7vSRXN3M2vQp"}},{"cell_type":"markdown","source":["* 579p 합성곱 특성맵 , padding 이야기 ,pooling 이야기\n","* 위치불변성 - 이동에 대한 불변성을 가진다\n","* CNN은 패턴(현재 시점)만 반영하고 DNN은 아예 반영을 안 하니 데이터의 추세를 반영하는 것은 RNN 사용(날씨, 주식 등)\n","* RNN : 짧은 맥락은 적용되지만 긴 추세는 고려하기 힘듦(말이 길어지면 vanishing 문제 또 발생)\n","* 이 문제를 해결하는 것이 LSTM(Long-Short-Term-Memory) <- cell state를 추가해서 중요한 것을 추가하는 방식(각 입력에서 cell에 기록할지 말지 결정)\n","* 당연히 시간이 오래 걸리므로 architecture를 간단하게 만든다(속도가 빠르다)\n","* 일방향, 양방향\n","* **순환 데이터**를 만드는 이유 <- 그룹핑을 하면 다음을 예상하기 더 쉬워진다(**묶인 데이터의 관계를 반영**하여 더 깊게 학습 가능)\n","*  regression은 그냥 묶어서 다음으로 넘기면 되는데 classification은 포함하는 지 아닌지에 따라서 좀 나뉜다\n"," * regression y_train의 경우 가장 많은 값(0,1)을 값으로 삼는다 -> 즉 단일값으로 나타나는 것이 맞다\n"," *"],"metadata":{"id":"evuCy_V26wML"}},{"cell_type":"markdown","source":["stock_data를 과제 할 때 참고할 것 <br>\n","IMDB<br>\n","constrast running <- 고양이 label로 퓨마 판별(퓨마의 loss가 고양이보다 크다) <br>\n","\n","encoding <br>\n","* one-hot encoding / multi-hot encoding\n","* 문자를 숫자로 인코딩\n","* 텍스트 인코딩\n","\n","과제는 나중에 사캠 참조할 것"],"metadata":{"id":"aFCpyHB7HW-e"}}]}