{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkp7z6K84Es6YfX16M/TEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksy3432/ai-class-DL-Application-/blob/main/%5Blecture%5DDLA_week_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI 공부법\n",
        "* 기본적인 부분은 석사,연구원으로 들어가던가 온라인 강의 등으로 시작 가능\n",
        "* 그 위의 것들은 icml,lclr,cupr,nlps,aaai 등의 학회지에서 논문 참고하면서 트렌드를 알아야 함\n",
        "* kaggle,githur,pacov 등 사이트에서 정보(경진대회 등) 제공\n",
        "* 먼저 research field를 정해야 한다"
      ],
      "metadata": {
        "id": "8k-KqEF4_w9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#그 동안 배운 것 리뷰\n",
        "* train과 test 데이터셋을 나누는 이유 = 중복을 피해서 일반화 성능 향상 모략\n",
        "* self-superviser learning = 모델이 알아서 label을 만들어서 훈련\n",
        "* semi-supervisor learning = supervisor learning과 unsupervisor learning을 합친 것 모델이나 데이터나 암튼 하나랑 다른 하나랑 합쳐서 사용\n",
        "* 노이즈나 학습을 입력데이터에 섞는 이유 = 오히려 도움이 될 수 있음\n",
        "* feature selection과 feature engineering = selection은 있는 것 중에 선택,  engineering은 있는 특성을 조합하여 새로운 특성 발견 등\n",
        "* underfitting,overfitting 방지법 :ov(normalization,reqularization,모델 축소,dropout,early_stopping 등)\n",
        "* 데이터 전처리 후 모델에서 다시 scaling하는 이유 = 가중치가 계속 달라져서 scale도 같이 계속 변경됨\n",
        "* uf(모델 복잡화, data 증식 등)\n",
        "*data argmentation : 이미지는 단순하지만 나머지는 어려움 대표적으로 smote등이 존재함\n",
        "*embedding = 모델이 이해할 수 있게 변환(1.one-hot encoding(데이터 간의 관계를 끊어줌), 2.vector encoding(다양한 데이터를 고정된 크기의 숫자 벡터로 변환))\n",
        "* 앙상블 = 모델 여러 개 돌려서 제일 좋은 거 채택(대표적으로 randomforest)\n",
        "* 샘플을 중복을 허용하여 뽑는다(bootstrap sample) 반대는 OOB\n",
        "* 하이퍼파라미터 튜닝 = 사용자가 조정할 수 있는 파라미터를 튜닝, 모델의 성능을 향상시키기 위해 사용된다. 조정하는 이유 = 모델 성능 최적화(최적의 가중치와 편향값을 얻기 위함)\n",
        "* ml에서는 한 번에 얻기 위해 gridsearchcv나 gradienthistcv등을 사용하기도 함\n",
        "*혼동 행렬(confusion matrix) 꼭 사용해야하는 이유 = 어느 클래스에서 분류를 잘 못 하는지 알 수 있다(클래스 간의 분류 확인 가능) 이를 통해 모델 성능 향상 도모\n",
        "* acc 대신 fn 쓰는 이유 = acc는 데이터 불균형 반영이 안됨\n",
        "*precision & recall = 양성 양성 /음성 음성(f1 score로 확인하기도 함)\n",
        "* 경사하강법 = 실제와 예측의 오차를 최대한 줄여서 최적의 가중치와 편향값 찾기\n",
        "* 확률적 경사하강법 : 보법이 다르다 샘플을 랜덤으로 뽑아서 가중치 찾기, 배치 경사 하강법 = 전체 배치의 샘플을 이용하여 가중치 찾기, 미니배치 : 전체 배치가 아닌 일부 샘플을 뽑아서 그룹핑한 후 미니 배치를 만들어서 거기서 가중치 찾기\n",
        "* svm = 주어진 샘플들을 잘 구분할 수 있는 선(최적의 결정 경계)을 구하는 것이 목적\n",
        "*지니불순도 = 임의로 선택한 두 샘플이 서로 다른 클래스로 분류될 확률, 같은 클래스를 가진 샘플이 많을수록 지니불순도가 낮아진다\n",
        "*엔트로피셤에 안 나옴\n",
        "*bootstarp과 bagging,pasting\n",
        "*차원의 저주 = 불필요한 특성들이 너무 많아서 성능이 저하됨(feature_selection 등으로 극복)\n",
        "* pca 등을 사용하는 이유 = 중요한 특성 뽑아서 압축하기 위함\n",
        "* 매니폴드 가설(manifold hypothesis) = 고차원의 데이터를 저차원으로 표현할 수 있다 => 그러면 학습하기가 쉽다\n",
        "* 활성화 함수(Activation Function) = 신경망의 출력을 비선형적으로 변환하여 복잡한 패턴을 학습할 수 있게 한다, 쓰지 않은 경우 f(x)만 쓰여서 단순해짐 그러므로 layer마다 비선형성을 주어 더 복잡한 특성을 추출하기 위함임\n",
        "* mlp의 계단형 함수 = gvp 문제 발생\n",
        "* gvp와 폭주, 해결방안\n",
        "* pretrained model 쓰는 이유 = 잘 만들어진 모델을 가져오면 성능이 이미 보장되어있고 튜닝을 하는 경우에도 훨씬 더 수월하다\n",
        "* freezing = 기존의 가중치 동결, 이후에 필요한 부분만 훈련\n",
        "*fine tunning = 약간만 고치는 거, pretrained model을 쓰는 경우 이미 많이 비슷하기에 훨씬 수월하다\n",
        "*ae,vae...등등 저번시간에 한 거라 안 함(시험엔 나옴)\n",
        "*cnn의 차이점 = local feature를 사용해서 패턴을 학습할 수 있다\n",
        "*pooling 역할 = 1. 정보 추출 , 2.공간 불변성\n",
        "* resnet = skip-connection을 통해 redusial learning을 한다. 즉, 출력값에 입력값을 다시 더해줘서 중간의 노드가 출력값과 입력값의 차이를 학습한다.(족보 보고 시험 공부하는 것과 비슷함) 출력에서 gradient vanishing의 발생을 저하시킨다.(이건 두 번째 장점) 이는 효과적인 학습을 할 수 있게 만들어준다.\n",
        "* inception = resnet에서 한 발짝 더 나가서 서로 다른 필터를 사용한다(1x1 사용 이유 3가지 알아둘 것)\n",
        "* resnet,senet...vae 등의 목적 = 경량화\n",
        "* 어떻게 경량화하면서도 성능을 높일 것인가?가 관건\n",
        "* 주요 키워드 = gvp,lightweight\n",
        "* depthwise separate convolution = 채널을 분리해서 개별로 중요도를 다르게 두자(attantion 개념 도입)\n",
        "* yolo, 객체 탐지는 일단 패스\n",
        "* rnn 도입 이유, lstm,gru 등등 다시 보기\n",
        "* attention = 중요한 것에 집중하자\n",
        "* tranformer(embedding,positional embedding,self-attention,cross-attention,mlti-head attention) = seq2seq 모델(seq를 넣어서 seq를 출력함)\n",
        "* scale-dot product attention = 스케일이 달라서 어느 하나가 너무 튄다! => 다른 걸 계산하기 힘드니 스케일을 맞춰주자, Attention 메커니즘에서 입력 벡터의 내적(유사도)을 구할 때 생기는 값의 크기를 조정하기 위해 사용\n",
        "* 루?옹 알아야하나??\n",
        "*attention 뒤에 마스크 처리해서 미래를 보고 예측하는 것을 방지\n",
        "* vision transformer\n",
        "*17장 diffusion 모델은 셤 안 나옴\n",
        "*요즘 트렌드 = llm + 강화학습\n",
        "*다음 주 시험 10시에 415호 1시간 필기 시험"
      ],
      "metadata": {
        "id": "NfO-6DLX9T-e"
      }
    }
  ]
}